{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoWorkflow Documentation","text":"<p>Welcome to the GeoWorkflow project documentation! This guide helps you understand, use, and contribute to the AfricaPolis geospatial data processing pipeline.</p>"},{"location":"#what-is-geoworkflow","title":"What is GeoWorkflow?","text":"<p>GeoWorkflow is a Python-based pipeline for processing geospatial data, specifically designed for the AfricaPolis project. It handles the complete workflow from raw data extraction to analysis-ready enriched datasets.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started - Installation, configuration, and quick start guides</li> <li>Project Guide - Understand the codebase structure and architecture</li> <li>Tutorials - Step-by-step examples and how-tos</li> <li>API Reference - Detailed API documentation (via Sphinx)</li> <li>Development - Contributing guidelines and testing</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>\ud83d\udce6 GitHub Repository</li> <li>\ud83d\udcda API Documentation (Sphinx)</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-stage Processing Pipeline - Extract, clip, align, enrich, visualize</li> <li>Modular Processors - Extensible architecture for custom processors</li> <li>Configuration-driven - YAML-based workflow definitions</li> <li>Progress Tracking - Rich console output with detailed metrics</li> <li>Error Handling - Comprehensive error reporting and recovery</li> </ul>"},{"location":"#at-a-glance","title":"At a Glance","text":"<p>```python from geoworkflow.core.pipeline import ProcessingPipeline</p>"},{"location":"#define-workflow-configuration","title":"Define workflow configuration","text":"<p>config = {     \"stages\": [\"clip\", \"align\", \"enrich\"],     \"source_dir\": \"data/raw\",     \"output_dir\": \"data/processed\" }</p>"},{"location":"#run-pipeline","title":"Run pipeline","text":"<p>pipeline = ProcessingPipeline(config) results = pipeline.run()</p>"},{"location":"#documentation-structure_1","title":"Documentation Structure","text":"<ul> <li>API Reference - Complete API docs (Sphinx)</li> </ul>"},{"location":"development/contributing/","title":"Contributing to GeoWorkflow","text":"<p>We welcome contributions! This guide will help you get started.</p>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/jacksonfloods/geoworkflow.git\ncd geoworkflow\n\n# Create development environment\nconda env create -f environment.yml\nconda activate geoworkflow\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\"\n\n# Set up pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"development/contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 88 characters (Black default)</li> <li>Use Google-style docstrings</li> <li>Type hints for function signatures</li> </ul>"},{"location":"development/contributing/#running-formatters","title":"Running Formatters","text":"<pre><code># Format code\nblack src/ tests/\n\n# Sort imports\nisort src/ tests/\n\n# Check types\nmypy src/geoworkflow\n\n# Lint\nflake8 src/ tests/\n</code></pre>"},{"location":"development/contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a branch <pre><code>git checkout -b feature/my-new-feature\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li>Write clear, documented code</li> <li>Add tests for new functionality</li> <li> <p>Update documentation as needed</p> </li> <li> <p>Test your changes <pre><code>pytest tests/ -v\npytest --cov=geoworkflow --cov-report=html\n</code></pre></p> </li> <li> <p>Commit your changes <pre><code>git add .\ngit commit -m \"Add feature: description\"\n</code></pre></p> </li> <li> <p>Push and create PR <pre><code>git push origin feature/my-new-feature\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":"<ul> <li>Write tests for all new processors</li> <li>Aim for &gt;80% code coverage</li> <li>Use pytest fixtures for common test data</li> <li>Test both success and failure cases</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom geoworkflow.processors.spatial.clipper import ClippingProcessor\n\ndef test_clipper_basic(tmp_path):\n    \"\"\"Test basic clipping functionality.\"\"\"\n    config = {\n        \"input_dir\": \"tests/fixtures/rasters\",\n        \"output_dir\": tmp_path,\n        \"boundary_file\": \"tests/fixtures/boundaries/test_aoi.geojson\"\n    }\n\n    processor = ClippingProcessor(config)\n    result = processor.process()\n\n    assert result.success\n    assert result.processed_count &gt; 0\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docstrings for any changed functions</li> <li>Add examples to documentation files</li> <li>Update README if adding major features</li> <li>Build docs locally to check formatting:   <pre><code>cd docs\nmake html\nmkdocs serve\n</code></pre></li> </ul>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all tests pass</li> <li>Update CHANGELOG.md</li> <li>Request review from maintainers</li> <li>Address review feedback</li> <li>Squash commits if requested</li> </ol>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<p>Open an issue or reach out to the maintainers.</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>GeoWorkflow uses pytest for testing.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=geoworkflow --cov-report=html\n\n# Run specific test file\npytest tests/test_aoi_processor.py\n\n# Run specific test\npytest tests/test_aoi_processor.py::test_aoi_creation\n\n# Run with verbose output\npytest -v\n\n# Run only fast tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures\n\u251c\u2500\u2500 fixtures/                # Test data\n\u2502   \u251c\u2500\u2500 rasters/\n\u2502   \u251c\u2500\u2500 vectors/\n\u2502   \u2514\u2500\u2500 configs/\n\u251c\u2500\u2500 unit/                    # Unit tests\n\u2502   \u251c\u2500\u2500 test_processors/\n\u2502   \u251c\u2500\u2500 test_utils/\n\u2502   \u2514\u2500\u2500 test_schemas/\n\u2514\u2500\u2500 integration/             # Integration tests\n    \u2514\u2500\u2500 test_pipelines/\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#basic-test-example","title":"Basic Test Example","text":"<pre><code>import pytest\nfrom pathlib import Path\nfrom geoworkflow.processors.aoi.processor import AOIProcessor\n\ndef test_aoi_processor_creates_output(tmp_path):\n    \"\"\"Test that AOI processor creates output file.\"\"\"\n    config = {\n        \"source_file\": \"tests/fixtures/boundaries.geojson\",\n        \"output_dir\": tmp_path,\n        \"countries\": [\"Ghana\"],\n        \"output_format\": \"geojson\"\n    }\n\n    processor = AOIProcessor(config)\n    result = processor.process()\n\n    assert result.success\n    assert len(result.output_paths) &gt; 0\n    assert result.output_paths[0].exists()\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code># conftest.py\n@pytest.fixture\ndef sample_raster(tmp_path):\n    \"\"\"Create a sample raster for testing.\"\"\"\n    import rasterio\n    import numpy as np\n\n    raster_path = tmp_path / \"test_raster.tif\"\n    data = np.random.rand(100, 100)\n\n    with rasterio.open(\n        raster_path, 'w',\n        driver='GTiff',\n        height=100, width=100,\n        count=1, dtype=data.dtype,\n        crs='EPSG:4326',\n        transform=rasterio.transform.from_bounds(0, 0, 1, 1, 100, 100)\n    ) as dst:\n        dst.write(data, 1)\n\n    return raster_path\n\n# test_file.py\ndef test_with_fixture(sample_raster):\n    \"\"\"Test using the fixture.\"\"\"\n    assert sample_raster.exists()\n</code></pre>"},{"location":"development/testing/#testing-errors","title":"Testing Errors","text":"<pre><code>def test_processor_invalid_config():\n    \"\"\"Test that invalid config raises error.\"\"\"\n    with pytest.raises(ValueError, match=\"Missing required\"):\n        processor = MyProcessor({})\n</code></pre>"},{"location":"development/testing/#parametrized-tests","title":"Parametrized Tests","text":"<pre><code>@pytest.mark.parametrize(\"crs,expected\", [\n    (\"EPSG:4326\", \"WGS84\"),\n    (\"EPSG:3857\", \"Web Mercator\"),\n])\ndef test_crs_names(crs, expected):\n    \"\"\"Test CRS name conversion.\"\"\"\n    result = get_crs_name(crs)\n    assert expected in result\n</code></pre>"},{"location":"development/testing/#test-markers","title":"Test Markers","text":"<pre><code># Mark slow tests\n@pytest.mark.slow\ndef test_large_dataset():\n    pass\n\n# Mark integration tests\n@pytest.mark.integration\ndef test_full_pipeline():\n    pass\n\n# Run only unit tests\npytest -m \"not integration\"\n</code></pre>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate HTML coverage report\npytest --cov=geoworkflow --cov-report=html\n\n# Open report\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":"<p>Tests run automatically on every PR via GitHub Actions.</p>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>GeoWorkflow uses YAML configuration files to define workflows and processor settings.</p>"},{"location":"getting-started/configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<pre><code># workflow_config.yaml\nworkflow:\n  name: \"AfricaPolis Processing\"\n  stages:\n    - extract\n    - clip\n    - align\n    - enrich\n\npaths:\n  source_dir: \"data/00_source\"\n  extracted_dir: \"data/01_extracted\"\n  clipped_dir: \"data/02_clipped\"\n  processed_dir: \"data/03_processed\"\n  output_dir: \"data/04_analysis_ready\"\n\naoi:\n  countries: [\"Ghana\", \"Togo\", \"Kenya\", \"Tanzania\"]\n  buffer_km: 50\n\nprocessing:\n  target_crs: \"EPSG:4326\"\n  resolution_m: 100\n  resampling_method: \"bilinear\"\n</code></pre>"},{"location":"getting-started/configuration/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from geoworkflow.core.config import ConfigManager\n\n# Load from file\nconfig = ConfigManager.load(\"config/workflows/my_workflow.yaml\")\n\n# Or create programmatically\nconfig = {\n    \"stages\": [\"clip\", \"align\"],\n    \"source_dir\": \"data/raw\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for Earth Engine integration:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\nexport GEE_PROJECT_ID=\"your-project-id\"\n</code></pre>"},{"location":"getting-started/configuration/#see-also","title":"See Also","text":"<ul> <li>Processor-specific configs</li> <li>Workflow examples</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Conda (recommended) or pip</li> <li>GDAL and spatial libraries</li> </ul>"},{"location":"getting-started/installation/#using-conda-recommended","title":"Using Conda (Recommended)","text":"<p>```bash</p>"},{"location":"getting-started/installation/#clone-repository","title":"Clone repository","text":"<p>git clone https://github.com/jacksonfloods/geoworkflow.git cd geoworkflow</p>"},{"location":"getting-started/installation/#create-environment","title":"Create environment","text":"<p>conda env create -f environment.yml conda activate geoworkflow</p>"},{"location":"getting-started/installation/#install-in-development-mode","title":"Install in development mode","text":"<p>pip install -e \".[dev]\"</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will get you up and running with GeoWorkflow in minutes.</p>"},{"location":"getting-started/quickstart/#basic-workflow-example","title":"Basic Workflow Example","text":"<pre><code>from geoworkflow.core.pipeline import ProcessingPipeline\nfrom pathlib import Path\n\n# Define your workflow configuration\nconfig = {\n    \"stages\": [\"extract\", \"clip\", \"align\"],\n    \"source_dir\": \"data/00_source\",\n    \"output_dir\": \"data/processed\",\n    \"countries\": [\"Ghana\", \"Togo\"]\n}\n\n# Create and run pipeline\npipeline = ProcessingPipeline(config)\nresults = pipeline.run()\n\n# Check results\nprint(f\"Processed {results.processed_count} files\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-individual-processors","title":"Using Individual Processors","text":"<pre><code>from geoworkflow.processors.spatial.clipper import ClippingProcessor\nfrom geoworkflow.schemas.config_models import ClippingConfig\n\n# Configure clipping\nconfig = ClippingConfig(\n    input_dir=Path(\"data/01_extracted\"),\n    output_dir=Path(\"data/02_clipped\"),\n    boundary_file=Path(\"data/aoi/ghana_boundary.geojson\")\n)\n\n# Run processor\nprocessor = ClippingProcessor(config)\nresult = processor.process()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration</li> <li>Explore Directory Structure</li> <li>Try a Complete Workflow Tutorial</li> </ul>"},{"location":"guide/concepts/","title":"Core Concepts","text":"<p>Understanding these key concepts will help you work effectively with GeoWorkflow.</p>"},{"location":"guide/concepts/#processors","title":"Processors","text":"<p>Processors are the building blocks of the workflow. Each processor handles a specific transformation:</p> <ul> <li>BaseProcessor - Abstract base class all processors inherit from</li> <li>AOIProcessor - Creates Areas of Interest from various sources</li> <li>ClippingProcessor - Spatially clips datasets to boundaries</li> <li>AlignmentProcessor - Ensures consistent CRS and resolution</li> <li>StatisticalEnrichmentProcessor - Calculates zonal statistics</li> </ul>"},{"location":"guide/concepts/#processing-results","title":"Processing Results","text":"<p>Every processor returns a standardized <code>ProcessingResult</code> object: ```python @dataclass class ProcessingResult:     success: bool     processed_count: int     failed_count: int     elapsed_time: float     output_paths: List[Path]     metadata: Dict[str, Any]</p>"},{"location":"guide/structure/","title":"Project Structure","text":"<p>This page documents the organization of the GeoWorkflow codebase.</p>"},{"location":"guide/structure/#source-code-layout","title":"Source Code Layout","text":"<pre><code>src/geoworkflow/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __version__.py\n\u2502   \u251c\u2500\u2500 visualization/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 raster/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 processor.py\n\u2502   \u2502   \u251c\u2500\u2500 vector/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 reports/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u251c\u2500\u2500 constants.py\n\u2502   \u2502   \u251c\u2500\u2500 enhanced_base.py\n\u2502   \u2502   \u251c\u2500\u2500 exceptions.py\n\u2502   \u2502   \u251c\u2500\u2500 logging_setup.py\n\u2502   \u2502   \u251c\u2500\u2500 pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 pipeline_enhancements.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 earth_engine_error_handler.py\n\u2502   \u2502   \u251c\u2500\u2500 earth_engine_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 file_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 mask_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 progress_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 raster_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 resource_utils.py\n\u2502   \u2502   \u251c\u2500\u2500 validation.py\n\u2502   \u251c\u2500\u2500 cli/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 cli_structure.py\n\u2502   \u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2502   \u251c\u2500\u2500 commands/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 aoi.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 extract.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pipeline.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 process.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 visualize.py\n\u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 config_models.py\n\u2502   \u251c\u2500\u2500 processors/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 integration/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 enrichment.py\n\u2502   \u2502   \u251c\u2500\u2500 spatial/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 aligner.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 clipper.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 masker.py\n\u2502   \u2502   \u251c\u2500\u2500 extraction/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 archive.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 open_buildings.py\n\u2502   \u2502   \u251c\u2500\u2500 aoi/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 processor.py\n</code></pre>"},{"location":"guide/structure/#directory-descriptions","title":"Directory Descriptions","text":""},{"location":"guide/structure/#core","title":"<code>core/</code>","text":"<p>Core modules - Foundation classes, base processors, configuration, and constants</p>"},{"location":"guide/structure/#processors","title":"<code>processors/</code>","text":"<p>Data processors - Specialized processors for each workflow stage</p>"},{"location":"guide/structure/#processorsaoi","title":"<code>processors/aoi/</code>","text":"<p>Area of Interest (AOI) creation and management</p>"},{"location":"guide/structure/#processorsspatial","title":"<code>processors/spatial/</code>","text":"<p>Spatial operations (clipping, alignment, reprojection)</p>"},{"location":"guide/structure/#processorsextraction","title":"<code>processors/extraction/</code>","text":"<p>Data extraction from archives and downloads</p>"},{"location":"guide/structure/#processorsintegration","title":"<code>processors/integration/</code>","text":"<p>Statistical enrichment and data integration</p>"},{"location":"guide/structure/#schemas","title":"<code>schemas/</code>","text":"<p>Configuration schemas - Pydantic models for validation</p>"},{"location":"guide/structure/#utils","title":"<code>utils/</code>","text":"<p>Utility modules - Helper functions and common operations</p>"},{"location":"guide/structure/#cli","title":"<code>cli/</code>","text":"<p>Command-line interface - Entry points for CLI commands</p>"},{"location":"guide/workflow/","title":"Workflow Stages","text":"<p>GeoWorkflow processes geospatial data through a series of well-defined stages.</p>"},{"location":"guide/workflow/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>Source Data \u2192 Extract \u2192 Clip \u2192 Align \u2192 Enrich \u2192 Visualize \u2192 Analysis-Ready\n</code></pre>"},{"location":"guide/workflow/#stage-descriptions","title":"Stage Descriptions","text":""},{"location":"guide/workflow/#1-extract","title":"1. Extract","text":"<p>Purpose: Extract data from archives and downloads</p> <p>Input: Compressed archives (ZIP, TAR.GZ) Output: Uncompressed geospatial files Processor: <code>ArchiveExtractionProcessor</code></p> <pre><code>from geoworkflow.processors.extraction.archive import ArchiveExtractionProcessor\n\nprocessor = ArchiveExtractionProcessor(config)\nresult = processor.process()\n</code></pre>"},{"location":"guide/workflow/#2-clip","title":"2. Clip","text":"<p>Purpose: Spatially subset data to area of interest</p> <p>Input: Large-extent rasters or vectors Output: Clipped data for specific regions Processor: <code>ClippingProcessor</code></p> <p>Use cases: - Extract country-specific data from global datasets - Focus processing on urban areas - Reduce file sizes for analysis</p>"},{"location":"guide/workflow/#3-align","title":"3. Align","text":"<p>Purpose: Ensure consistent spatial properties</p> <p>Input: Misaligned rasters (different CRS, resolution, extent) Output: Aligned rasters ready for analysis Processor: <code>AlignmentProcessor</code></p> <p>Operations: - Reproject to common CRS - Resample to common resolution - Match extents and pixel grids</p>"},{"location":"guide/workflow/#4-enrich","title":"4. Enrich","text":"<p>Purpose: Calculate zonal statistics and integrate datasets</p> <p>Input: Vector boundaries + raster data Output: Enriched vector with statistical attributes Processor: <code>StatisticalEnrichmentProcessor</code></p> <p>Calculated metrics: - Mean, min, max, std - Percentiles - Counts and sums</p>"},{"location":"guide/workflow/#5-visualize","title":"5. Visualize","text":"<p>Purpose: Generate maps and charts</p> <p>Input: Analysis-ready data Output: PNG/PDF maps, interactive HTML Processor: <code>VisualizationProcessor</code></p>"},{"location":"guide/workflow/#running-multi-stage-workflows","title":"Running Multi-Stage Workflows","text":"<pre><code>from geoworkflow.core.pipeline import ProcessingPipeline\n\nconfig = {\n    \"stages\": [\"extract\", \"clip\", \"align\", \"enrich\"],\n    \"source_dir\": \"data/00_source\",\n    # ... other config\n}\n\npipeline = ProcessingPipeline(config)\nresults = pipeline.run()\n\n# Or run stages individually\npipeline.run_stage(\"clip\")\npipeline.run_stage(\"align\")\n</code></pre>"},{"location":"guide/workflow/#stage-dependencies","title":"Stage Dependencies","text":"<p>Each stage expects specific inputs:</p> Stage Requires Produces Extract Archives Raw files Clip Raw files + boundaries Clipped files Align Clipped rasters Aligned rasters Enrich Aligned rasters + vectors Enriched vectors Visualize Enriched data Maps/charts"},{"location":"tutorials/basic-workflow/","title":"Basic Workflow Tutorial","text":"<p>This tutorial walks through a complete data processing workflow using GeoWorkflow.</p>"},{"location":"tutorials/basic-workflow/#scenario","title":"Scenario","text":"<p>Process PM2.5 air quality data for Ghana and Togo, enriching urban boundary polygons with pollution statistics.</p>"},{"location":"tutorials/basic-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>GeoWorkflow installed</li> <li>Sample data in <code>data/00_source/</code></li> <li>AFRICAPOLIS boundaries</li> </ul>"},{"location":"tutorials/basic-workflow/#step-1-create-area-of-interest","title":"Step 1: Create Area of Interest","text":"<pre><code>from geoworkflow.processors.aoi.processor import AOIProcessor\nfrom geoworkflow.schemas.config_models import AOIConfig\nfrom pathlib import Path\n\n# Configure AOI creation\naoi_config = AOIConfig(\n    source_file=Path(\"data/00_source/boundaries/africa_boundaries.geojson\"),\n    output_dir=Path(\"data/aoi\"),\n    countries=[\"Ghana\", \"Togo\"],\n    buffer_km=50,\n    output_format=\"geojson\"\n)\n\n# Create AOI\naoi_processor = AOIProcessor(aoi_config)\nresult = aoi_processor.process()\n\nprint(f\"Created AOI: {result.output_paths[0]}\")\n</code></pre>"},{"location":"tutorials/basic-workflow/#step-2-extract-archives","title":"Step 2: Extract Archives","text":"<pre><code>from geoworkflow.processors.extraction.archive import ArchiveExtractionProcessor\nfrom geoworkflow.schemas.config_models import ExtractionConfig\n\nextract_config = ExtractionConfig(\n    source_dir=Path(\"data/00_source/archives/pm25\"),\n    output_dir=Path(\"data/01_extracted/pm25\"),\n    archive_format=\"zip\"\n)\n\nextractor = ArchiveExtractionProcessor(extract_config)\nresult = extractor.process()\n</code></pre>"},{"location":"tutorials/basic-workflow/#step-3-clip-to-aoi","title":"Step 3: Clip to AOI","text":"<pre><code>from geoworkflow.processors.spatial.clipper import ClippingProcessor\nfrom geoworkflow.schemas.config_models import ClippingConfig\n\nclip_config = ClippingConfig(\n    input_dir=Path(\"data/01_extracted/pm25\"),\n    output_dir=Path(\"data/02_clipped/pm25\"),\n    boundary_file=Path(\"data/aoi/ghana_togo_aoi.geojson\"),\n    maintain_extent=False\n)\n\nclipper = ClippingProcessor(clip_config)\nresult = clipper.process()\n</code></pre>"},{"location":"tutorials/basic-workflow/#step-4-align-rasters","title":"Step 4: Align Rasters","text":"<pre><code>from geoworkflow.processors.spatial.aligner import AlignmentProcessor\nfrom geoworkflow.schemas.config_models import AlignmentConfig\n\nalign_config = AlignmentConfig(\n    input_dir=Path(\"data/02_clipped/pm25\"),\n    output_dir=Path(\"data/03_processed/pm25\"),\n    target_crs=\"EPSG:4326\",\n    resolution=(0.001, 0.001),  # ~100m at equator\n    resampling_method=\"bilinear\"\n)\n\naligner = AlignmentProcessor(align_config)\nresult = aligner.process()\n</code></pre>"},{"location":"tutorials/basic-workflow/#step-5-enrich-urban-boundaries","title":"Step 5: Enrich Urban Boundaries","text":"<pre><code>from geoworkflow.processors.integration.enrichment import StatisticalEnrichmentProcessor\nfrom geoworkflow.schemas.config_models import StatisticalEnrichmentConfig\n\nenrich_config = StatisticalEnrichmentConfig(\n    vector_file=Path(\"data/01_extracted/AFRICAPOLIS2020.geojson\"),\n    raster_dir=Path(\"data/03_processed/pm25\"),\n    output_dir=Path(\"data/04_analysis_ready\"),\n    statistics=[\"mean\", \"std\", \"min\", \"max\"],\n    prefix=\"pm25\"\n)\n\nenricher = StatisticalEnrichmentProcessor(enrich_config)\nresult = enricher.process()\n\nprint(f\"Enriched {result.processed_count} urban areas\")\n</code></pre>"},{"location":"tutorials/basic-workflow/#step-6-examine-results","title":"Step 6: Examine Results","text":"<pre><code>import geopandas as gpd\n\n# Load enriched data\ngdf = gpd.read_file(\"data/04_analysis_ready/africapolis_pm25_stats.geojson\")\n\n# View statistics\nprint(gdf[[\"AgglomName\", \"pm25_mean\", \"pm25_std\"]].head())\n\n# Simple visualization\ngdf.plot(column=\"pm25_mean\", legend=True, figsize=(12, 8))\n</code></pre>"},{"location":"tutorials/basic-workflow/#complete-pipeline-version","title":"Complete Pipeline Version","text":"<p>Or run everything at once:</p> <pre><code>from geoworkflow.core.pipeline import ProcessingPipeline\n\npipeline_config = {\n    \"stages\": [\"extract\", \"clip\", \"align\", \"enrich\"],\n    \"source_dir\": Path(\"data/00_source\"),\n    \"countries\": [\"Ghana\", \"Togo\"],\n    \"target_crs\": \"EPSG:4326\",\n    \"resolution_m\": 100\n}\n\npipeline = ProcessingPipeline(pipeline_config)\nresults = pipeline.run()\n\nfor stage, result in results.items():\n    print(f\"{stage}: {result.processed_count} files processed\")\n</code></pre>"},{"location":"tutorials/basic-workflow/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Files not found Solution: Check that paths are correct and files exist</p> <p>Issue: CRS mismatch warnings Solution: This is expected - the alignment stage handles it</p> <p>Issue: Memory errors with large rasters Solution: Process smaller regions or use windowed reading</p>"},{"location":"tutorials/custom-processors/","title":"Creating Custom Processors","text":"<p>Learn how to extend GeoWorkflow with custom processors.</p>"},{"location":"tutorials/custom-processors/#basic-processor-structure","title":"Basic Processor Structure","text":"<p>All processors inherit from <code>BaseProcessor</code>:</p> <pre><code>from geoworkflow.core.base import BaseProcessor, ProcessingResult\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nclass MyCustomProcessor(BaseProcessor):\n    \"\"\"Custom processor for specific transformation.\"\"\"\n\n    def __init__(self, config: Dict[str, Any], logger=None):\n        super().__init__(config, logger)\n        # Custom initialization\n\n    def _validate_config(self, config: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate processor configuration.\"\"\"\n        required_keys = [\"input_dir\", \"output_dir\"]\n        for key in required_keys:\n            if key not in config:\n                raise ValueError(f\"Missing required config: {key}\")\n        return config\n\n    def process(self) -&gt; ProcessingResult:\n        \"\"\"Execute the processing logic.\"\"\"\n        self._start_processing()\n\n        # Your processing logic here\n        processed_count = 0\n\n        try:\n            # Process files\n            processed_count = self._do_processing()\n\n            result = ProcessingResult(\n                success=True,\n                processed_count=processed_count,\n                elapsed_time=self._elapsed_time()\n            )\n        except Exception as e:\n            self.logger.error(f\"Processing failed: {e}\")\n            result = ProcessingResult(success=False, message=str(e))\n\n        return result\n\n    def _do_processing(self) -&gt; int:\n        \"\"\"Implement your actual processing logic.\"\"\"\n        # Example: process all GeoJSON files\n        count = 0\n        input_dir = Path(self.config[\"input_dir\"])\n\n        for geojson_file in input_dir.glob(\"*.geojson\"):\n            # Process each file\n            self._process_file(geojson_file)\n            count += 1\n\n        return count\n</code></pre>"},{"location":"tutorials/custom-processors/#example-custom-filter-processor","title":"Example: Custom Filter Processor","text":"<pre><code>import geopandas as gpd\nfrom geoworkflow.core.base import BaseProcessor, ProcessingResult\n\nclass AttributeFilterProcessor(BaseProcessor):\n    \"\"\"Filter vector features by attribute values.\"\"\"\n\n    def _validate_config(self, config):\n        required = [\"input_file\", \"output_file\", \"filter_column\", \"filter_values\"]\n        for key in required:\n            if key not in config:\n                raise ValueError(f\"Missing: {key}\")\n        return config\n\n    def process(self) -&gt; ProcessingResult:\n        self._start_processing()\n\n        try:\n            # Load data\n            gdf = gpd.read_file(self.config[\"input_file\"])\n\n            # Apply filter\n            column = self.config[\"filter_column\"]\n            values = self.config[\"filter_values\"]\n            filtered = gdf[gdf[column].isin(values)]\n\n            # Save result\n            filtered.to_file(self.config[\"output_file\"])\n\n            return ProcessingResult(\n                success=True,\n                processed_count=len(filtered),\n                elapsed_time=self._elapsed_time(),\n                message=f\"Filtered to {len(filtered)} features\"\n            )\n        except Exception as e:\n            self.logger.error(f\"Filter failed: {e}\")\n            return ProcessingResult(success=False, message=str(e))\n\n# Usage\nconfig = {\n    \"input_file\": \"data/cities.geojson\",\n    \"output_file\": \"data/large_cities.geojson\",\n    \"filter_column\": \"population\",\n    \"filter_values\": [100000, 500000, 1000000]\n}\n\nprocessor = AttributeFilterProcessor(config)\nresult = processor.process()\n</code></pre>"},{"location":"tutorials/custom-processors/#integrating-with-pipeline","title":"Integrating with Pipeline","text":"<p>Register your processor:</p> <pre><code>from geoworkflow.core.pipeline import ProcessingPipeline\n\n# Register custom processor\npipeline = ProcessingPipeline(config)\npipeline.register_processor(\"filter\", AttributeFilterProcessor)\n\n# Use in workflow\nworkflow_config = {\n    \"stages\": [\"filter\", \"clip\", \"align\"],\n    # ...\n}\npipeline.run()\n</code></pre>"},{"location":"tutorials/custom-processors/#best-practices","title":"Best Practices","text":"<ol> <li>Always validate configuration in <code>_validate_config()</code></li> <li>Use ProcessingResult for consistent return values</li> <li>Log important events using <code>self.logger</code></li> <li>Handle errors gracefully with try/except</li> <li>Document your processor with clear docstrings</li> <li>Test thoroughly with various input types</li> </ol>"}]}